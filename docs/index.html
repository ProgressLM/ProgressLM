<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ProgressLM: Towards Progress Reasoning in Vision-Language Models">
    <meta name="keywords" content="Progress Reasoning, Vision-Language Models, VLM, Machine Learning, Deep Learning">
    <meta name="author" content="Jianshu Zhang, Chengxuan Qian, Haosen Sun, Haoran Lu, Dingcheng Wang, Letian Xue, Han Liu">

    <!-- Open Graph -->
    <meta property="og:title" content="ProgressLM">
    <meta property="og:description" content="Towards Progress Reasoning in Vision-Language Models">
    <meta property="og:type" content="website">

    <title>ProgressLM - Progress Reasoning in Vision-Language Models</title>

    <!-- Stylesheets -->
    <link rel="stylesheet" href="css/style.css">

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <!-- Hero Section -->
    <section id="hero">
        <div class="container">
            <div class="title-with-icon">
                <img src="imgs/icon.png" alt="ProgressLM Icon" class="title-icon">
                <h1>ProgressLM</h1>
            </div>
            <p class="subtitle">Towards Progress Reasoning in Vision-Language Models</p>

            <div class="buttons">
                <a href="#" class="btn btn-primary">
                    <i class="fas fa-file-pdf"></i> Paper
                </a>
                <a href="https://github.com/ProgressLM/ProgressLM" class="btn" target="_blank">
                    <i class="fab fa-github"></i> Code
                </a>
                <a href="https://huggingface.co/Raymond-Qiancx/ProgressLM-3B-SFT" class="btn" target="_blank">
                    <i class="fas fa-cube"></i> SFT Model
                </a>
                <a href="https://huggingface.co/Raymond-Qiancx/ProgressLM-3B-RL" class="btn" target="_blank">
                    <i class="fas fa-cube"></i> RL Model
                </a>
                <a href="https://huggingface.co/datasets/Raymond-Qiancx/ProgressLM-Dataset" class="btn" target="_blank">
                    <i class="fas fa-database"></i> Dataset
                </a>
            </div>

            <div class="authors-box">
                <div class="authors">
                    <span class="author">Jianshu Zhang<sup>*</sup></span>
                    <span class="author">Chengxuan Qian<sup>*</sup></span>
                    <span class="author">Haosen Sun</span>
                    <span class="author">Haoran Lu</span>
                    <span class="author">Dingcheng Wang</span>
                    <span class="author">Letian Xue</span>
                    <span class="author">Han Liu</span>
                </div>
                <p class="equal-contribution"><sup>*</sup> Equal Contribution</p>
            </div>

            <div class="teaser-image">
                <img src="imgs/teaser.png" alt="ProgressLM Teaser" style="width: 100%; max-width: 900px; margin-top: 32px; border-radius: 8px;">
            </div>
        </div>
    </section>

    <!-- Abstract Section -->
    <section id="abstract">
        <div class="container">
            <h2>Abstract</h2>
            <p>
                Given a task demonstration and a single observation, the goal is to estimate <strong>how much of the task has already been completed</strong>.
                Direct prediction can often judge whether the task is unfinished, but struggles to assign a well-calibrated progress score.
                Progress reasoning instead follows a coarse-to-fine process: it first performs <strong>episodic retrieval</strong> to coarsely locate the observation along the demonstrated task,
                then applies <strong>mental simulation</strong> to imagine the transition from the retrieved anchor to the current observation, enabling a fine-grained estimate of completed progress.
            </p>
            <p>
                <em>Can vision-language models acquire progress estimation as a general reasoning capability from a single observation?</em>
                To systematically study this question, we select robotic manipulation tasks as a controlled and representative domain,
                where task execution exhibits clear, interpretable, and temporally ordered progressions.
                Each instance provides a task <strong>demonstration</strong> and a single <strong>observation</strong>,
                and the model is required to predict a normalized <strong>progress score</strong> indicating how far the task has progressed.
            </p>
        </div>
    </section>

    <!-- Benchmark Design Section -->
    <section id="benchmark-design">
        <div class="container">
            <h2>Benchmark Design</h2>
            <p style="margin-bottom: 24px; color: var(--text-secondary); text-align: center;">
                We design our benchmark along three key dimensions to comprehensively evaluate progress reasoning capabilities.
            </p>
            <div class="benchmark-design-image">
                <img src="imgs/bench_curation.png" alt="Benchmark Design" style="width: 100%; max-width: 1000px; margin: 0 auto; display: block; border-radius: 8px;">
            </div>
            <div class="dimension-cards" style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 20px; margin-top: 32px;">
                <div class="dimension-card" style="background: var(--card-bg); padding: 20px; border-radius: 8px; border: 1px solid var(--border-color);">
                    <h3 style="color: var(--accent-color); margin-bottom: 8px;"><i class="fas fa-eye"></i> Demonstration Modality</h3>
                    <p style="color: var(--text-secondary);">Text-based vs. vision-based demonstrations to test different input modalities.</p>
                </div>
                <div class="dimension-card" style="background: var(--card-bg); padding: 20px; border-radius: 8px; border: 1px solid var(--border-color);">
                    <h3 style="color: var(--accent-color); margin-bottom: 8px;"><i class="fas fa-camera"></i> Viewpoint Correspondence</h3>
                    <p style="color: var(--text-secondary);">Same-view vs. cross-view settings to evaluate spatial reasoning robustness.</p>
                </div>
                <div class="dimension-card" style="background: var(--card-bg); padding: 20px; border-radius: 8px; border: 1px solid var(--border-color);">
                    <h3 style="color: var(--accent-color); margin-bottom: 8px;"><i class="fas fa-question-circle"></i> Answerability</h3>
                    <p style="color: var(--text-secondary);">Well-defined vs. ambiguous cases to test the model's ability to recognize unanswerable queries.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Method Section -->
    <section id="method">
        <div class="container">
            <h2>Method</h2>
            <div class="placeholder" style="margin-bottom: 32px;">
                <i class="fas fa-diagram-project"></i>
                <p>Method overview figure coming soon...</p>
            </div>

            <div class="method-overview">
                <div class="method-card">
                    <h3><i class="fas fa-graduation-cap"></i> SFT Training</h3>
                    <p>
                        Supervised fine-tuning using LLaMA-Factory framework.
                        Supports LoRA, QLoRA, and full fine-tuning with Qwen2.5-VL and Qwen3-VL architectures.
                    </p>
                </div>
                <div class="method-card">
                    <h3><i class="fas fa-brain"></i> RL Training (GRPO)</h3>
                    <p>
                        Reinforcement learning with Group Relative Policy Optimization using EasyR1 framework.
                        Provides distributed training support with FSDP.
                    </p>
                </div>
                <div class="method-card">
                    <h3><i class="fas fa-chart-line"></i> Evaluation</h3>
                    <p>
                        Comprehensive evaluation pipeline with multiple metrics including VOC, Score Error,
                        Ref Error, and N/A Recall for progress reasoning assessment.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- Benchmarks Section -->
    <section id="benchmarks">
        <div class="container">
            <h2>Benchmarks</h2>

            <div class="statistics-image" style="margin-bottom: 32px;">
                <img src="imgs/statistics.png" alt="Dataset Statistics" style="width: 100%; max-width: 900px; margin: 0 auto; display: block; border-radius: 8px;">
            </div>

            <div class="benchmark-grid">
                <div class="benchmark-category">
                    <h3><i class="fas fa-code"></i> prog-bench</h3>
                    <p style="margin-bottom: 16px; color: var(--text-secondary);">
                        Programmatic task benchmarks for progress reasoning evaluation
                    </p>
                    <ul>
                        <li><i class="fas fa-check"></i> Text Demo (Normal)</li>
                        <li><i class="fas fa-check"></i> Text Demo (Unanswerable)</li>
                        <li><i class="fas fa-check"></i> Visual Demo (Same View)</li>
                        <li><i class="fas fa-check"></i> Visual Demo (Cross View)</li>
                        <li><i class="fas fa-check"></i> Visual Demo (Unanswerable)</li>
                    </ul>
                </div>

                <div class="benchmark-category">
                    <h3><i class="fas fa-person-walking"></i> human-bench</h3>
                    <p style="margin-bottom: 16px; color: var(--text-secondary);">
                        Human activity benchmarks for progress reasoning evaluation
                    </p>
                    <ul>
                        <li><i class="fas fa-check"></i> Text Demo (Human Activities)</li>
                        <li><i class="fas fa-check"></i> Visual Demo (Human Activities)</li>
                    </ul>
                </div>
            </div>

            <div class="metrics">
                <h3><i class="fas fa-ruler"></i> Evaluation Metrics</h3>
                <div class="metrics-grid">
                    <div class="metric-item">
                        <strong>VOC</strong>
                        <span>Trajectory Order Consistency</span>
                    </div>
                    <div class="metric-item">
                        <strong>Score Error</strong>
                        <span>Normalized Progress Score Error</span>
                    </div>
                    <div class="metric-item">
                        <strong>Ref Error</strong>
                        <span>Reference Step Index Error</span>
                    </div>
                    <div class="metric-item">
                        <strong>N/A Recall</strong>
                        <span>Unanswerable Detection Recall</span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Results Section -->
    <section id="results">
        <div class="container">
            <h2>Supported Models</h2>
            <p style="margin-bottom: 24px; color: var(--text-secondary); text-align: center;">
                We provide evaluation scripts for a wide range of vision-language models.
            </p>
            <div class="models-list">
                <div class="model-badge">
                    <i class="fas fa-robot"></i> Qwen2.5-VL (3B, 7B, 32B, 72B)
                </div>
                <div class="model-badge">
                    <i class="fas fa-robot"></i> Qwen3-VL (2B, 4B, 8B, 32B, 30B-MoE)
                </div>
                <div class="model-badge">
                    <i class="fas fa-robot"></i> InternVL
                </div>
                <div class="model-badge">
                    <i class="fas fa-robot"></i> GPT-4V / GPT-4o
                </div>
            </div>
            <p style="margin-top: 16px; color: var(--text-secondary); text-align: center; font-size: 0.9em;">
                Qwen3-VL supports both <strong>thinking</strong> and <strong>non-thinking</strong> modes for evaluation.
            </p>
        </div>
    </section>

    <!-- Citation Section -->
    <section id="citation">
        <div class="container">
            <h2>Citation</h2>
            <p>If you find our work useful, please consider citing:</p>
            <div class="bibtex-container">
                <pre id="bibtex-content">@article{zhang2025progresslm,
  title={ProgressLM: Towards Progress Reasoning in Vision-Language Models},
  author={Zhang, Jianshu and Qian, Chengxuan and Sun, Haosen and Lu, Haoran and Wang, Dingcheng and Xue, Letian and Liu, Han},
  journal={arXiv preprint arXiv:2505.XXXXX},
  year={2025}
}</pre>
                <button id="copy-btn"><i class="fas fa-copy"></i> Copy</button>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p>MIT License - Copyright 2025 ProgressLM Team</p>
            <p>
                <a href="https://github.com/ProgressLM/ProgressLM" target="_blank">
                    <i class="fab fa-github"></i> GitHub Repository
                </a>
            </p>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="js/main.js"></script>
</body>
</html>
