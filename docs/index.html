<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ProgressLM: Towards Progress Reasoning in Vision-Language Models">
    <meta name="keywords" content="Progress Reasoning, Vision-Language Models, VLM, Machine Learning, Deep Learning">
    <meta name="author" content="Jianshu Zhang, Chengxuan Qian, Haosen Sun, Haoran Lu, Dingcheng Wang, Letian Xue, Han Liu">

    <!-- Open Graph -->
    <meta property="og:title" content="ProgressLM">
    <meta property="og:description" content="Towards Progress Reasoning in Vision-Language Models">
    <meta property="og:type" content="website">

    <title>ProgressLM - Progress Reasoning in Vision-Language Models</title>

    <!-- Favicon -->
    <link rel="icon" type="image/png" href="imgs/icon.png">
    <link rel="apple-touch-icon" href="imgs/icon.png">

    <!-- Stylesheets -->
    <link rel="stylesheet" href="css/style.css?v=7">

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <!-- Hero Section -->
    <section id="hero">
        <div class="container">
            <div class="title-with-icon">
                <img src="imgs/icon.png" alt="ProgressLM Icon" class="title-icon">
                <h1>ProgressLM</h1>
            </div>
            <p class="subtitle">Towards Progress Reasoning in Vision-Language Models</p>

            <div class="buttons">
                <a href="#" class="btn btn-primary">
                    <i class="fas fa-file-pdf"></i> Paper
                </a>
                <a href="https://github.com/ProgressLM/ProgressLM" class="btn" target="_blank">
                    <i class="fab fa-github"></i> Code
                </a>
                <a href="https://huggingface.co/Raymond-Qiancx/ProgressLM-3B-SFT" class="btn" target="_blank">
                    <i class="fas fa-cube"></i> SFT Model
                </a>
                <a href="https://huggingface.co/Raymond-Qiancx/ProgressLM-3B-RL" class="btn" target="_blank">
                    <i class="fas fa-cube"></i> RL Model
                </a>
                <a href="https://huggingface.co/datasets/Raymond-Qiancx/ProgressLM-Dataset" class="btn" target="_blank">
                    <i class="fas fa-database"></i> Dataset
                </a>
            </div>

            <div class="authors-box">
                <div class="authors">
                    <span class="author"><a href="https://sterzhang.github.io/" target="_blank">Jianshu Zhang</a><sup>*</sup></span>
                    <span class="author"><a href="https://qiancx.com/" target="_blank">Chengxuan Qian</a><sup>*</sup></span>
                    <span class="author">Haosen Sun</span>
                    <span class="author">Haoran Lu</span>
                    <span class="author">Dingcheng Wang</span>
                    <span class="author">Letian Xue</span>
                    <span class="author"><a href="https://magics.cs.northwestern.edu/people.html" target="_blank">Han Liu</a></span>
                </div>
                <p class="equal-contribution"><sup>*</sup> Equal Contribution</p>
            </div>

            <div class="teaser-image">
                <img src="imgs/teaser.png" alt="ProgressLM Teaser" style="width: 100%; max-width: 900px; margin-top: 32px; border-radius: 8px;">
                <p class="teaser-caption">
                    <strong><em>Can vision-language models acquire progress estimation as a general reasoning capability from a single observation?</em></strong> Given a task demonstration and a single observation, the goal is to estimate how much of the task has already been completed.
                    Direct prediction can often judge whether the task is unfinished, but struggles to assign a well-calibrated progress score.
                    Progress reasoning instead follows a coarse-to-fine process: it first performs <em>episodic retrieval</em> to coarsely locate the observation along the demonstrated task,
                    then applies <em>mental simulation</em> to imagine the transition from the retrieved anchor to the current observation, enabling a fine-grained estimate of completed progress.
                </p>
            </div>
        </div>
    </section>

    <!-- Abstract Section -->
    <section id="abstract">
        <div class="container">
            <h2>Abstract</h2>
            <p>
                Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. In this work, we introduce <strong>Progress-Bench</strong>, a benchmark for systematically evaluating progress reasoning in VLMs. We further explore a human-inspired two-stage progress reasoning paradigm that combines <strong>episodic retrieval</strong> with <strong>mental simulation</strong>. We instantiate this paradigm through both training-free prompting and a training-based approach built on an automatically curated dataset, <strong>ProgressLM-45K</strong>. Evaluating 14 VLMs on Progress-Bench, we find that current models struggle to reliably estimate task progress. While training-free prompting that enforces structured progress reasoning yields improvements, these gains are limited and model-dependent. In contrast, <strong>ProgressLM-3B</strong> achieves consistent improvements in accuracy, robustness to viewpoint variation, and calibrated handling of unanswerable cases, even at small model scale. Further analyses reveal characteristic error patterns of existing VLMs and clarify when and why progress reasoning succeeds or fails.
            </p>
        </div>
    </section>

    <!-- Progress Annotation Section -->
    <section id="progress-annotation">
        <div class="container">
            <details class="collapsible-section" open>
                <summary class="collapsible-title">
                    <span>How do we annotate progress under controlled shifts?</span>
                    <span class="collapse-icon"></span>
                </summary>
                <div class="collapsible-content">
                    <div class="annotation-image" style="margin-bottom: 24px;">
                        <img src="imgs/bench_curation.png" alt="Progress Annotation" style="width: 100%; border-radius: 8px;">
                    </div>
                    <p style="text-align: justify; line-height: 1.8;">
                        We construct progress data by pairing each task demonstration with a single observation sampled from intermediate or boundary moments during execution, and assign its progress label via temporal interpolation between adjacent key steps. To systematically probe progress reasoning beyond static matching, we introduce controlled shifts along three dimensions: <strong>demonstration modality</strong> (vision-based vs. text-based), <strong>viewpoint correspondence</strong> (same-view vs. cross-view for vision demos), and <strong>answerability</strong>, where semantic mismatches are injected so that progress becomes ill-defined and the correct output is N/A.
                    </p>
                </div>
            </details>
        </div>
    </section>

    <!-- Data Scaling Section -->
    <section id="data-scaling">
        <div class="container">
            <details class="collapsible-section" open>
                <summary class="collapsible-title">
                    <span>Scaling progress data across robots and objects</span>
                    <span class="collapse-icon"></span>
                </summary>
                <div class="collapsible-content">
                    <div class="scaling-images" style="margin-bottom: 24px;">
                        <img src="imgs/statistics.png" alt="Statistics" style="width: 100%; border-radius: 8px; margin-bottom: 16px;">
                        <img src="imgs/bar-stat-combined.png" alt="Bar Statistics" style="width: 100%; border-radius: 8px; margin-bottom: 16px;">
                        <img src="imgs/sankey.png" alt="Sankey Diagram" style="width: 100%; border-radius: 8px;">
                    </div>
                    <p style="text-align: justify; line-height: 1.8;">
                        To scale progress reasoning beyond a single embodiment or environment, we curate progress estimation data spanning single-arm manipulators, dual-arm systems, and humanoid robots, together with diverse object interactions. <strong>Progress-Bench</strong> evaluates models on thousands of observation queries grounded in real manipulation trajectories, while <strong>ProgressLM-45K</strong> further expands training coverage with large-scale supervised and reinforcement learning samples. This diversity encourages models to learn transferable progress cues—tracking long-horizon state evolution rather than overfitting to specific robots, viewpoints, or object appearances.
                    </p>
                </div>
            </details>
        </div>
    </section>

    <!-- Method Section -->
    <section id="method">
        <div class="container">
            <h2>Method</h2>

            <div class="method-overview">
                <div class="method-card">
                    <h3><i class="fas fa-bolt"></i> Direct Prediction</h3>
                    <p>
                        The model directly outputs a progress percentage from the demonstration and the current observation, without any explicit reasoning decomposition.
                    </p>
                </div>
                <div class="method-card">
                    <h3><i class="fas fa-comments"></i> Training-free Approach</h3>
                    <p>
                        We enforce a human-inspired two-stage progress reasoning process via structured prompting, where the model first performs <strong>episodic retrieval</strong> to locate a coarse anchor step, then applies <strong>mental simulation</strong> to infer fine-grained progress.
                    </p>
                </div>
                <div class="method-card">
                    <h3><i class="fas fa-graduation-cap"></i> SFT</h3>
                    <p>
                        We explicitly train the model to internalize episodic retrieval and mental simulation using <strong>cold-start supervised fine-tuning</strong> on <strong>ProgressLM-25K-CoT</strong> via <a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank">LLaMA-Factory</a>.
                    </p>
                </div>
                <div class="method-card">
                    <h3><i class="fas fa-brain"></i> RL</h3>
                    <p>
                        We further refine the SFT model with <strong>GRPO-based reinforcement learning</strong> on <strong>ProgressLM-20K-RL</strong> via <a href="https://github.com/hiyouga/EasyR1" target="_blank">EasyR1</a> and <a href="https://github.com/volcengine/verl" target="_blank">verl</a>, yielding <strong>ProgressLM</strong> with improved accuracy, robustness, and calibrated handling of unanswerable cases.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- Performance Section -->
    <section id="performance">
        <div class="container">
            <details class="collapsible-section" open>
                <summary class="collapsible-title">
                    <span>Performance on Answerable Scenarios</span>
                    <span class="collapse-icon"></span>
                </summary>
                <div class="collapsible-content">
                    <!-- Subsection 1 -->
                    <div class="result-subsection">
                        <h4 class="subsection-title">How well do current VLMs perform at progress estimation?</h4>
                        <div class="subsection-content">
                            <img src="imgs/Tab1.png" alt="Table 1: Performance on Answerable Scenarios" style="width: 100%; border-radius: 8px; margin-bottom: 16px;">
                            <p>Overall, current VLMs exhibit limited and highly variable progress estimation ability under direct prediction. Even strong models achieve only moderate performance and remain highly sensitive to the demonstration setting, while several models show abnormally low or even negative PRC, indicating distorted temporal ordering rather than meaningful ordinal progress reasoning. In addition, some models adopt overly conservative behaviors with elevated AFRR, rejecting answerable cases instead of producing calibrated progress scores.</p>
                        </div>
                    </div>

                    <!-- Subsection 2 -->
                    <div class="result-subsection">
                        <h4 class="subsection-title">Does training-free progress reasoning help?</h4>
                        <div class="subsection-content">
                            <p>The training-free approach provides only conditional benefits: it tends to help large models improve temporal consistency (often reflected in PRC), but its effect is unstable and can be neutral or harmful for smaller models. This suggests that limited-capacity VLMs may imitate the structured format without truly improving progress understanding, leading to degraded pointwise accuracy (higher NSE) or increased rejection behavior (higher AFRR).</p>
                        </div>
                    </div>

                    <!-- Subsection 3 -->
                    <div class="result-subsection">
                        <h4 class="subsection-title">Does training-based progress reasoning help?</h4>
                        <div class="subsection-content">
                            <p>Yes—explicit learning yields consistent and substantial improvements even at small model scale. Compared to the base model, PROGRESSLM improves both single-point progress accuracy (lower NSE) and trajectory-level temporal consistency (higher PRC), demonstrating that robust progress reasoning is not merely a consequence of scale. Notably, the reinforced variant further strengthens overall reliability, showing that progress reasoning can be effectively learned through targeted supervision and optimization rather than relying on prompting alone.</p>
                        </div>
                    </div>
                </div>
            </details>
        </div>
    </section>

    <!-- Results Section -->
    <section id="results">
        <div class="container">
            <h2>Supported Models</h2>
            <p style="margin-bottom: 24px; color: var(--text-secondary); text-align: center;">
                We conduct extensive validation on open-source VLMs across different architectures and model scales.
            </p>
            <div class="models-list">
                <div class="model-badge">
                    <i class="fas fa-robot"></i> Qwen2.5-VL (3B, 7B, 32B, 72B)
                </div>
                <div class="model-badge">
                    <i class="fas fa-robot"></i> Qwen3-VL (2B, 4B, 8B, 32B)
                </div>
                <div class="model-badge">
                    <i class="fas fa-robot"></i> InternVL3.5 (4B, 8B, 14B, 38B)
                </div>
                <div class="model-badge">
                    <i class="fas fa-robot"></i> GPT-5 / GPT-5-mini
                </div>
            </div>
            <p style="margin-top: 16px; color: var(--text-secondary); text-align: center; font-size: 0.9em;">
                All models support <strong>direct prediction</strong>, <strong>non-thinking</strong>, and <strong>thinking</strong> modes for evaluation.
            </p>
        </div>
    </section>

    <!-- Citation Section -->
    <section id="citation">
        <div class="container">
            <h2>Citation</h2>
            <p>If you find our work useful, please consider citing:</p>
            <div class="bibtex-container">
                <pre id="bibtex-content">@article{zhang2025progresslm,
  title={ProgressLM: Towards Progress Reasoning in Vision-Language Models},
  author={Zhang, Jianshu and Qian, Chengxuan and Sun, Haosen and Lu, Haoran and Wang, Dingcheng and Xue, Letian and Liu, Han},
  journal={arXiv preprint arXiv:2505.XXXXX},
  year={2025}
}</pre>
                <button id="copy-btn"><i class="fas fa-copy"></i> Copy</button>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p>MIT License - Copyright 2025 ProgressLM Team</p>
            <p>
                <a href="https://github.com/ProgressLM/ProgressLM" target="_blank">
                    <i class="fab fa-github"></i> GitHub Repository
                </a>
            </p>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="js/main.js"></script>
</body>
</html>
