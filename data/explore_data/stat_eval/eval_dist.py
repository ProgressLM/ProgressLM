import json
import pandas as pd
from collections import defaultdict
import numpy as np

def analyze_jsonl(file_path):
    """
    åˆ†æJSONLæ–‡ä»¶ï¼Œç»Ÿè®¡trajectoryå’Œsampleçš„å„ç§æŒ‡æ ‡
    """
    # è¯»å–æ•°æ®
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    
    print(f"{'='*80}")
    print(f"æ•°æ®é›†ç»Ÿè®¡åˆ†ææŠ¥å‘Š")
    print(f"{'='*80}\n")
    
    # åŸºæœ¬ç»Ÿè®¡
    total_samples = len(data)
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples}\n")
    
    # è§£æIDç»“æ„
    parsed_data = []
    for item in data:
        id_parts = item['id'].split('/')
        if len(id_parts) == 3:
            parsed_data.append({
                'id': item['id'],
                'source_short': id_parts[0],
                'action_type': id_parts[1],
                'trajectory_id': id_parts[2],
                'data_source': item['data_source']
            })
    
    df = pd.DataFrame(parsed_data)
    
    # 1. ç»Ÿè®¡æ¯ä¸ªdata_sourceçš„trajectoryæ•°é‡
    print(f"{'='*80}")
    print(f"1. æ¯ä¸ªdata_sourceçš„trajectoryæ•°é‡")
    print(f"{'='*80}")
    
    trajectories_per_source = df.groupby('data_source')['id'].nunique()
    for source, count in trajectories_per_source.items():
        print(f"  - {source}: {count} ä¸ªtrajectories")
    print()
    
    # 2. ç»Ÿè®¡æ¯ä¸ªdata_sourceçš„æ ·æœ¬æ•°é‡
    print(f"{'='*80}")
    print(f"2. æ¯ä¸ªdata_sourceçš„æ ·æœ¬æ•°é‡")
    print(f"{'='*80}")
    
    samples_per_source = df.groupby('data_source').size()
    for source, count in samples_per_source.items():
        print(f"  - {source}: {count} ä¸ªsamples")
    print()
    
    # 3. è®¡ç®—æ¯ä¸ªtrajectoryçš„sampleæ•°é‡ï¼Œå¹¶è¿›è¡Œäº”å€¼ç»Ÿè®¡
    print(f"{'='*80}")
    print(f"3. æ¯ä¸ªtrajectoryçš„sampleæ•°é‡ç»Ÿè®¡")
    print(f"{'='*80}")
    
    samples_per_trajectory = df.groupby(['data_source', 'id']).size()
    
    # æŒ‰data_sourceåˆ†ç»„è¿›è¡Œäº”å€¼ç»Ÿè®¡
    for source in df['data_source'].unique():
        source_df = df[df['data_source'] == source]
        samples_per_traj = source_df.groupby('id').size()
        
        print(f"\n  {source}:")
        print(f"    - Trajectoryæ•°é‡: {len(samples_per_traj)}")
        print(f"    - æ ·æœ¬æ•°é‡: {len(source_df)}")
        print(f"    - å¹³å‡æ¯ä¸ªtrajectoryçš„æ ·æœ¬æ•°: {samples_per_traj.mean():.2f}")
        print(f"    - äº”å€¼ç»Ÿè®¡:")
        print(f"      â€¢ æœ€å°å€¼: {samples_per_traj.min()}")
        print(f"      â€¢ ç¬¬ä¸€å››åˆ†ä½æ•°(Q1): {samples_per_traj.quantile(0.25):.2f}")
        print(f"      â€¢ ä¸­ä½æ•°(Median): {samples_per_traj.median():.2f}")
        print(f"      â€¢ ç¬¬ä¸‰å››åˆ†ä½æ•°(Q3): {samples_per_traj.quantile(0.75):.2f}")
        print(f"      â€¢ æœ€å¤§å€¼: {samples_per_traj.max()}")
    
    # 4. ç»Ÿè®¡sourceç®€ç§°
    print(f"\n{'='*80}")
    print(f"4. Sourceç®€ç§°ç»Ÿè®¡")
    print(f"{'='*80}")
    
    source_short_counts = df.groupby('source_short')['id'].nunique()
    for source_short, count in source_short_counts.items():
        print(f"  - {source_short}: {count} ä¸ªtrajectories")
    print()
    
    # 5. ç»Ÿè®¡åŠ¨ä½œç±»å‹
    print(f"{'='*80}")
    print(f"5. åŠ¨ä½œç±»å‹(Action Type)ç»Ÿè®¡")
    print(f"{'='*80}")
    
    action_type_counts = df.groupby('action_type')['id'].nunique()
    print(f"  æ€»å…± {len(action_type_counts)} ç§åŠ¨ä½œç±»å‹:\n")
    for action_type, count in sorted(action_type_counts.items(), key=lambda x: x[1], reverse=True):
        print(f"  - {action_type}: {count} ä¸ªtrajectories")
    print()
    
    # 6. ç»Ÿè®¡æ¯ä¸ªsource_shortä¸‹çš„åŠ¨ä½œç±»å‹
    print(f"{'='*80}")
    print(f"6. æ¯ä¸ªsourceç®€ç§°ä¸‹çš„åŠ¨ä½œç±»å‹ç»Ÿè®¡")
    print(f"{'='*80}")
    
    for source_short in df['source_short'].unique():
        source_df = df[df['source_short'] == source_short]
        action_types = source_df.groupby('action_type')['id'].nunique()
        print(f"\n  {source_short} ({len(action_types)} ç§åŠ¨ä½œ):")
        for action_type, count in sorted(action_types.items(), key=lambda x: x[1], reverse=True):
            print(f"    - {action_type}: {count} ä¸ªtrajectories")
    
    # 7. äº¤å‰ç»Ÿè®¡ï¼šsource_short Ã— action_type
    print(f"\n{'='*80}")
    print(f"7. Sourceç®€ç§° Ã— åŠ¨ä½œç±»å‹ äº¤å‰ç»Ÿè®¡è¡¨")
    print(f"{'='*80}\n")
    
    cross_tab = pd.crosstab(df['source_short'], df['action_type'], 
                            values=df['id'], aggfunc='nunique', margins=True)
    print(cross_tab)
    print()
    
    # 8. æ€»ä½“äº”å€¼ç»Ÿè®¡
    print(f"{'='*80}")
    print(f"8. å…¨å±€ç»Ÿè®¡ï¼šæ¯ä¸ªtrajectoryçš„æ ·æœ¬æ•°")
    print(f"{'='*80}")
    
    all_samples_per_traj = df.groupby('id').size()
    print(f"  - æ€»trajectoryæ•°: {len(all_samples_per_traj)}")
    print(f"  - æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"  - å¹³å‡æ¯ä¸ªtrajectoryçš„æ ·æœ¬æ•°: {all_samples_per_traj.mean():.2f}")
    print(f"  - äº”å€¼ç»Ÿè®¡:")
    print(f"    â€¢ æœ€å°å€¼: {all_samples_per_traj.min()}")
    print(f"    â€¢ ç¬¬ä¸€å››åˆ†ä½æ•°(Q1): {all_samples_per_traj.quantile(0.25):.2f}")
    print(f"    â€¢ ä¸­ä½æ•°(Median): {all_samples_per_traj.median():.2f}")
    print(f"    â€¢ ç¬¬ä¸‰å››åˆ†ä½æ•°(Q3): {all_samples_per_traj.quantile(0.75):.2f}")
    print(f"    â€¢ æœ€å¤§å€¼: {all_samples_per_traj.max()}")
    
    print(f"\n{'='*80}")
    print(f"åˆ†æå®Œæˆï¼")
    print(f"{'='*80}\n")
    
    return df

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python analyze_jsonl.py <jsonl_file_path>")
        print("ç¤ºä¾‹: python analyze_jsonl.py data.jsonl")
        sys.exit(1)
    
    file_path = sys.argv[1]
    df = analyze_jsonl(file_path)

# python /projects/b1222/userdata/jianshu/chengxuan/ProgressLM/data/explore_data/stat_eval/eval_dist.py /projects/b1222/userdata/jianshu/chengxuan/ProgressLM/data/eval/visual/visual_eval_all.jsonl