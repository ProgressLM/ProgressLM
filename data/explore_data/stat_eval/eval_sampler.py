import json
import pandas as pd
import numpy as np
from collections import defaultdict
import random

def stratified_sampling_trajectories(input_file, output_file, target_samples=3000, 
                                     stratify_by='data_source', random_seed=42):
    """
    å¯¹JSONLæ–‡ä»¶è¿›è¡Œåˆ†å±‚é‡‡æ ·ï¼Œä»¥trajectoryä¸ºå•ä½è¿›è¡Œé‡‡æ ·
    
    å‚æ•°:
        input_file: è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºJSONLæ–‡ä»¶è·¯å¾„
        target_samples: ç›®æ ‡æ ·æœ¬æ•°é‡ï¼ˆçº¦æ•°ï¼‰
        stratify_by: åˆ†å±‚ä¾æ® ('data_source', 'action_type', 'both')
        random_seed: éšæœºç§å­
    """
    random.seed(random_seed)
    np.random.seed(random_seed)
    
    # è¯»å–æ•°æ®
    print(f"{'='*80}")
    print(f"å¼€å§‹åˆ†å±‚é‡‡æ ·...")
    print(f"{'='*80}\n")
    
    data = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    
    # è§£æIDç»“æ„
    parsed_data = []
    for item in data:
        id_parts = item['id'].split('/')
        if len(id_parts) == 3:
            parsed_data.append({
                **item,
                'source_short': id_parts[0],
                'action_type': id_parts[1],
                'trajectory_id': id_parts[2]
            })
    
    df = pd.DataFrame(parsed_data)
    
    total_samples = len(df)
    total_trajectories = df['id'].nunique()
    target_ratio = target_samples / total_samples
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"  - æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"  - æ€»trajectoryæ•°: {total_trajectories}")
    print(f"  - ç›®æ ‡æ ·æœ¬æ•°: {target_samples}")
    print(f"  - ç›®æ ‡é‡‡æ ·æ¯”ä¾‹: {target_ratio:.2%}\n")
    
    # è®¡ç®—æ¯ä¸ªtrajectoryçš„æ ·æœ¬æ•°
    traj_sample_counts = df.groupby('id').size().to_dict()
    
    # æ ¹æ®åˆ†å±‚ä¾æ®è¿›è¡Œé‡‡æ ·
    if stratify_by == 'data_source':
        sampled_trajectories = _sample_by_single_column(
            df, 'data_source', target_ratio, traj_sample_counts
        )
    elif stratify_by == 'action_type':
        sampled_trajectories = _sample_by_single_column(
            df, 'action_type', target_ratio, traj_sample_counts
        )
    elif stratify_by == 'both':
        sampled_trajectories = _sample_by_both_columns(
            df, target_ratio, traj_sample_counts
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„åˆ†å±‚æ–¹å¼: {stratify_by}")
    
    # ç­›é€‰é‡‡æ ·çš„æ•°æ®
    sampled_df = df[df['id'].isin(sampled_trajectories)].copy()
    
    # æ‰“å°é‡‡æ ·åçš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆåœ¨ç§»é™¤è¾…åŠ©åˆ—ä¹‹å‰ï¼‰
    print(f"\n{'='*80}")
    print(f"âœ… é‡‡æ ·å®Œæˆï¼")
    print(f"{'='*80}\n")
    
    print(f"ğŸ“Š é‡‡æ ·åæ•°æ®ç»Ÿè®¡:")
    print(f"  - é‡‡æ ·æ ·æœ¬æ•°: {len(sampled_df)}")
    print(f"  - é‡‡æ ·trajectoryæ•°: {sampled_df['id'].nunique()}")
    print(f"  - å®é™…é‡‡æ ·æ¯”ä¾‹: {len(sampled_df)/total_samples:.2%}")
    print(f"  - è¾¾æˆç‡: {len(sampled_df)/target_samples:.2%}\n")
    
    # å¯¹æ¯”åŸå§‹åˆ†å¸ƒå’Œé‡‡æ ·ååˆ†å¸ƒï¼ˆåœ¨ç§»é™¤è¾…åŠ©åˆ—ä¹‹å‰ï¼‰
    _compare_distributions(df, sampled_df, stratify_by)
    
    # ç§»é™¤è¾…åŠ©åˆ—
    sampled_df = sampled_df.drop(['source_short', 'action_type', 'trajectory_id'], axis=1)
    
    # ä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        for _, row in sampled_df.iterrows():
            json.dump(row.to_dict(), f, ensure_ascii=False)
            f.write('\n')
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}\n")
    
    return sampled_df


def _sample_by_single_column(df, column, target_ratio, traj_sample_counts):
    """æŒ‰å•åˆ—åˆ†å±‚é‡‡æ ·"""
    sampled_trajectories = set()
    
    print(f"ğŸ“Œ æŒ‰ {column} è¿›è¡Œåˆ†å±‚é‡‡æ ·:\n")
    
    # è·å–æ¯ä¸ªåˆ†å±‚çš„trajectoryåˆ—è¡¨
    groups = df.groupby(column)['id'].unique()
    
    for group_name, trajectories in groups.items():
        # è®¡ç®—è¿™ä¸ªåˆ†å±‚åº”è¯¥é‡‡æ ·å¤šå°‘ä¸ªtrajectory
        group_total_samples = sum(traj_sample_counts[traj] for traj in trajectories)
        target_group_samples = int(group_total_samples * target_ratio)
        
        # è´ªå©ªé‡‡æ ·ï¼šæŒ‰trajectoryå¤§å°æ’åºï¼Œå°½å¯èƒ½æ¥è¿‘ç›®æ ‡
        traj_sizes = [(traj, traj_sample_counts[traj]) for traj in trajectories]
        selected = _greedy_sample_trajectories(traj_sizes, target_group_samples)
        
        sampled_trajectories.update(selected)
        
        actual_samples = sum(traj_sample_counts[traj] for traj in selected)
        print(f"  {group_name}:")
        print(f"    - åŸå§‹: {len(trajectories)} trajectories, {group_total_samples} samples")
        print(f"    - ç›®æ ‡: ~{target_group_samples} samples")
        print(f"    - é‡‡æ ·: {len(selected)} trajectories, {actual_samples} samples")
    
    return sampled_trajectories


def _sample_by_both_columns(df, target_ratio, traj_sample_counts):
    """æŒ‰data_sourceå’Œaction_typeä¸¤åˆ—è¿›è¡Œåˆ†å±‚é‡‡æ ·"""
    sampled_trajectories = set()
    
    print(f"ğŸ“Œ æŒ‰ data_source Ã— action_type è¿›è¡Œåˆ†å±‚é‡‡æ ·:\n")
    
    # è·å–æ¯ä¸ªç»„åˆçš„trajectoryåˆ—è¡¨
    groups = df.groupby(['data_source', 'action_type'])['id'].unique()
    
    for (data_source, action_type), trajectories in groups.items():
        # è®¡ç®—è¿™ä¸ªåˆ†å±‚åº”è¯¥é‡‡æ ·å¤šå°‘ä¸ªæ ·æœ¬
        group_total_samples = sum(traj_sample_counts[traj] for traj in trajectories)
        target_group_samples = int(group_total_samples * target_ratio)
        
        # è´ªå©ªé‡‡æ ·
        traj_sizes = [(traj, traj_sample_counts[traj]) for traj in trajectories]
        selected = _greedy_sample_trajectories(traj_sizes, target_group_samples)
        
        sampled_trajectories.update(selected)
        
        actual_samples = sum(traj_sample_counts[traj] for traj in selected)
        print(f"  {data_source} Ã— {action_type}:")
        print(f"    åŸå§‹: {len(trajectories)} traj, {group_total_samples} samples â†’ "
              f"é‡‡æ ·: {len(selected)} traj, {actual_samples} samples")
    
    return sampled_trajectories


def _greedy_sample_trajectories(traj_sizes, target_samples):
    """
    è´ªå©ªç®—æ³•é€‰æ‹©trajectoryï¼Œä½¿æ€»æ ·æœ¬æ•°å°½å¯èƒ½æ¥è¿‘ç›®æ ‡
    
    ç­–ç•¥ï¼š
    1. éšæœºæ‰“ä¹±trajectoryé¡ºåº
    2. æŒ‰é¡ºåºæ·»åŠ trajectoryï¼Œç›´åˆ°æ¥è¿‘æˆ–è¶…è¿‡ç›®æ ‡
    3. å¦‚æœè¶…è¿‡å¤ªå¤šï¼Œå°è¯•å»æ‰æœ€åä¸€ä¸ªï¼Œçœ‹å“ªä¸ªæ›´æ¥è¿‘ç›®æ ‡
    """
    random.shuffle(traj_sizes)
    
    selected = []
    current_samples = 0
    
    for traj, size in traj_sizes:
        if current_samples + size <= target_samples * 1.2:  # å…è®¸20%çš„è¶…å‡º
            selected.append(traj)
            current_samples += size
        elif current_samples < target_samples * 0.8:  # å¦‚æœè¿˜å·®å¾ˆå¤šï¼Œç»§ç»­æ·»åŠ 
            selected.append(traj)
            current_samples += size
    
    # å¦‚æœæ²¡æœ‰é€‰ä¸­ä»»ä½•trajectoryï¼Œè‡³å°‘é€‰ä¸€ä¸ª
    if not selected and traj_sizes:
        selected.append(traj_sizes[0][0])
    
    return selected


def _compare_distributions(original_df, sampled_df, stratify_by):
    """å¯¹æ¯”åŸå§‹åˆ†å¸ƒå’Œé‡‡æ ·ååˆ†å¸ƒ"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š åˆ†å¸ƒå¯¹æ¯”")
    print(f"{'='*80}\n")
    
    # æŒ‰data_sourceå¯¹æ¯”
    print("æŒ‰ data_source çš„åˆ†å¸ƒå¯¹æ¯”:")
    print(f"{'':30} {'åŸå§‹':<20} {'é‡‡æ ·å':<20} {'å·®å¼‚':<15}")
    print(f"{'-'*85}")
    
    orig_dist = original_df.groupby('data_source').size()
    samp_dist = sampled_df.groupby('data_source').size()
    
    for source in orig_dist.index:
        orig_count = orig_dist[source]
        samp_count = samp_dist.get(source, 0)
        orig_ratio = orig_count / len(original_df)
        samp_ratio = samp_count / len(sampled_df)
        diff = samp_ratio - orig_ratio
        
        print(f"{source:30} "
              f"{orig_count:6d} ({orig_ratio:6.2%})   "
              f"{samp_count:6d} ({samp_ratio:6.2%})   "
              f"{diff:+.2%}")
    
    # å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯¹æ¯”action_type
    if stratify_by in ['action_type', 'both']:
        print(f"\næŒ‰ action_type çš„åˆ†å¸ƒå¯¹æ¯” (å‰10ä¸ª):")
        print(f"{'':30} {'åŸå§‹':<20} {'é‡‡æ ·å':<20} {'å·®å¼‚':<15}")
        print(f"{'-'*85}")
        
        orig_dist_action = original_df.groupby('action_type').size().sort_values(ascending=False)
        samp_dist_action = sampled_df.groupby('action_type').size()
        
        for i, action in enumerate(orig_dist_action.head(10).index):
            orig_count = orig_dist_action[action]
            samp_count = samp_dist_action.get(action, 0)
            orig_ratio = orig_count / len(original_df)
            samp_ratio = samp_count / len(sampled_df) if len(sampled_df) > 0 else 0
            diff = samp_ratio - orig_ratio
            
            print(f"{action:30} "
                  f"{orig_count:6d} ({orig_ratio:6.2%})   "
                  f"{samp_count:6d} ({samp_ratio:6.2%})   "
                  f"{diff:+.2%}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 3:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python sample_dataset.py <input_file> <output_file> [target_samples] [stratify_by] [random_seed]")
        print("\nå‚æ•°è¯´æ˜:")
        print("  input_file     : è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„")
        print("  output_file    : è¾“å‡ºJSONLæ–‡ä»¶è·¯å¾„")
        print("  target_samples : ç›®æ ‡æ ·æœ¬æ•° (é»˜è®¤: 3000)")
        print("  stratify_by    : åˆ†å±‚æ–¹å¼ - 'data_source', 'action_type', 'both' (é»˜è®¤: 'data_source')")
        print("  random_seed    : éšæœºç§å­ (é»˜è®¤: 42)")
        print("\nç¤ºä¾‹:")
        print("  python sample_dataset.py data.jsonl sampled_data.jsonl 3000 data_source 42")
        print("  python sample_dataset.py data.jsonl sampled_data.jsonl 3000 both")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_file = sys.argv[2]
    target_samples = int(sys.argv[3]) if len(sys.argv) > 3 else 3000
    stratify_by = sys.argv[4] if len(sys.argv) > 4 else 'data_source'
    random_seed = int(sys.argv[5]) if len(sys.argv) > 5 else 42
    
    stratified_sampling_trajectories(
        input_file, 
        output_file, 
        target_samples=target_samples,
        stratify_by=stratify_by,
        random_seed=random_seed
    )

# python /projects/b1222/userdata/jianshu/chengxuan/ProgressLM/data/explore_data/stat_eval/eval_sampler.py /projects/b1222/userdata/jianshu/chengxuan/ProgressLM/data/eval/visual/visual_eval_all.jsonl /projects/b1222/userdata/jianshu/chengxuan/ProgressLM/data/eval/visual/visual_eval_3k.jsonl 3000 both 42