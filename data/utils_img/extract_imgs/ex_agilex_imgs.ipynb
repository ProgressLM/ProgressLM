{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8276aebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– è¯»å–JSONLæ–‡ä»¶: /home/runsheng/personal_3/qiancx/ProgressLM/data/raw/converted/h5_agilex_3rgb_converted.jsonl\n",
      "ğŸ“Š å‘ç° 2262 ä¸ªå”¯ä¸€IDéœ€è¦å¤„ç†\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æå–å›¾åƒ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2262/2262 [00:52<00:00, 42.85ID/s, æˆåŠŸ=2262, å¤±è´¥=0, å½“å‰=9_appleyellowplate_2...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… å¤„ç†å®Œæˆï¼\n",
      "   æˆåŠŸ: 2262 ä¸ªID\n",
      "   å¤±è´¥: 0 ä¸ªID\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ImageExtractor:\n",
    "    \"\"\"ä»JSONLæ–‡ä»¶è¯»å–ä¿¡æ¯å¹¶ä»HDF5æ–‡ä»¶ä¸­æå–å¯¹åº”å›¾åƒ\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_data_path: str = \"/home/runsheng/personal_3/qiancx/Sources/datasets/robomind/benchmark1_0_compressed\",\n",
    "                 output_base_path: str = \"/home/runsheng/personal_3/qiancx/Sources/datasets/robomind/images\"):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æå–å™¨\n",
    "        \n",
    "        Args:\n",
    "            base_data_path: HDF5æ•°æ®çš„åŸºç¡€è·¯å¾„\n",
    "            output_base_path: è¾“å‡ºå›¾åƒçš„åŸºç¡€è·¯å¾„\n",
    "        \"\"\"\n",
    "        self.base_data_path = Path(base_data_path)\n",
    "        self.output_base_path = Path(output_base_path)\n",
    "        self.errors = []  # è®°å½•é”™è¯¯\n",
    "        \n",
    "    def parse_id(self, id_string: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        è§£æIDå­—ç¬¦ä¸²\n",
    "        \n",
    "        Args:\n",
    "            id_string: æ ¼å¼å¦‚ \"h5_franka_3rgb/place_in_block_in_plate_1/1014_163457\"\n",
    "            \n",
    "        Returns:\n",
    "            åŒ…å«è§£æç»“æœçš„å­—å…¸\n",
    "        \"\"\"\n",
    "        parts = id_string.split('/')\n",
    "        if len(parts) < 3:\n",
    "            raise ValueError(f\"æ— æ•ˆçš„IDæ ¼å¼: {id_string}\")\n",
    "            \n",
    "        return {\n",
    "            'embodiment': parts[0],  # h5_franka_3rgb\n",
    "            'task_name': parts[1],    # place_in_block_in_plate_1\n",
    "            'episode_id': parts[2]     # 1014_163457\n",
    "        }\n",
    "    \n",
    "    def construct_h5_path(self, id_info: Dict[str, str]) -> Path:\n",
    "        \"\"\"\n",
    "        æ ¹æ®IDä¿¡æ¯æ„å»ºHDF5æ–‡ä»¶è·¯å¾„\n",
    "        \n",
    "        Args:\n",
    "            id_info: åŒ…å«embodiment, task_name, episode_idçš„å­—å…¸\n",
    "            \n",
    "        Returns:\n",
    "            HDF5æ–‡ä»¶çš„å®Œæ•´è·¯å¾„\n",
    "        \"\"\"\n",
    "        h5_dir = self.base_data_path / id_info['embodiment'] / id_info['task_name'] / \\\n",
    "                 'success_episodes' / 'train' / id_info['episode_id'] / 'data'\n",
    "        \n",
    "        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨\n",
    "        if not h5_dir.exists():\n",
    "            raise FileNotFoundError(f\"ç›®å½•ä¸å­˜åœ¨: {h5_dir}\")\n",
    "        \n",
    "        # æŸ¥æ‰¾HDF5æ–‡ä»¶ï¼ˆé€šå¸¸åä¸ºtrajectory.hdf5ï¼‰\n",
    "        h5_files = list(h5_dir.glob('*.hdf5'))\n",
    "        if not h5_files:\n",
    "            raise FileNotFoundError(f\"æœªæ‰¾åˆ°HDF5æ–‡ä»¶åœ¨: {h5_dir}\")\n",
    "        \n",
    "        # ä¼˜å…ˆé€‰æ‹©trajectory.hdf5\n",
    "        for h5_file in h5_files:\n",
    "            if h5_file.name == 'trajectory.hdf5':\n",
    "                return h5_file\n",
    "        \n",
    "        return h5_files[0]  # å¦‚æœæ²¡æœ‰trajectory.hdf5ï¼Œè¿”å›ç¬¬ä¸€ä¸ªHDF5æ–‡ä»¶\n",
    "    \n",
    "    def decode_image(self, compressed_image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        è§£ç å‹ç¼©çš„å›¾åƒæ•°æ®\n",
    "        \n",
    "        Args:\n",
    "            compressed_image: å‹ç¼©çš„å›¾åƒæ•°æ®\n",
    "            \n",
    "        Returns:\n",
    "            è§£ç åçš„RGBå›¾åƒ\n",
    "        \"\"\"\n",
    "        if compressed_image is None:\n",
    "            return None\n",
    "            \n",
    "        # å¦‚æœæ˜¯å•ä¸ªå›¾åƒ\n",
    "        if len(compressed_image.shape) == 1:\n",
    "            rgb = cv2.imdecode(compressed_image, cv2.IMREAD_COLOR)\n",
    "            if rgb is None:\n",
    "                # å°è¯•ç›´æ¥è§£æä¸ºæ•°ç»„\n",
    "                rgb = np.frombuffer(compressed_image, dtype=np.uint8)\n",
    "                # æ ¹æ®å¤§å°åˆ¤æ–­å›¾åƒå°ºå¯¸\n",
    "                if rgb.size == 2764800:\n",
    "                    rgb = rgb.reshape(720, 1280, 3)\n",
    "                elif rgb.size == 921600:\n",
    "                    rgb = rgb.reshape(480, 640, 3)\n",
    "            return rgb\n",
    "        else:\n",
    "            # å¦‚æœæ˜¯æ‰¹é‡å›¾åƒï¼Œè¿”å›Noneï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰\n",
    "            return None\n",
    "    \n",
    "    def extract_specific_frames(self, h5_path: Path, frame_names: List[str]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        ä»HDF5æ–‡ä»¶ä¸­æå–ç‰¹å®šå¸§ï¼ˆè‡ªåŠ¨ä»å¸§åè¯†åˆ«ç›¸æœºï¼‰\n",
    "        \n",
    "        Args:\n",
    "            h5_path: HDF5æ–‡ä»¶è·¯å¾„\n",
    "            frame_names: è¦æå–çš„å¸§åç§°åˆ—è¡¨ï¼ˆå¦‚ [\"camera_front_0000.jpg\", \"camera_front_0281.jpg\"]ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "            å¸§åç§°åˆ°å›¾åƒæ•°ç»„çš„æ˜ å°„\n",
    "        \"\"\"\n",
    "        images = {}\n",
    "        \n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            # æ£€æŸ¥æ˜¯å¦å‹ç¼©\n",
    "            is_compress = f.attrs.get('compress', True)\n",
    "            \n",
    "            # æ£€æŸ¥RGBå›¾åƒè·¯å¾„\n",
    "            if 'observations' not in f or 'rgb_images' not in f['observations']:\n",
    "                return images\n",
    "            \n",
    "            rgb_images_group = f['observations']['rgb_images']\n",
    "            \n",
    "            # è§£æå¸§åå¹¶æå–å›¾åƒ\n",
    "            for frame_name in frame_names:\n",
    "                try:\n",
    "                    # ä»å¸§åæå–ç›¸æœºåç§°å’Œç´¢å¼•\n",
    "                    # \"camera_front_0000.jpg\" -> camera=\"camera_front\", idx=0\n",
    "                    # \"camera_left_wrist_0281.jpg\" -> camera=\"camera_left_wrist\", idx=281\n",
    "                    parts = frame_name.split('_')\n",
    "                    frame_idx_str = parts[-1].split('.')[0]  # \"0000\"\n",
    "                    camera_name = '_'.join(parts[:-1])        # \"camera_front\"\n",
    "                    frame_idx = int(frame_idx_str)\n",
    "                    \n",
    "                    # æ£€æŸ¥ç›¸æœºæ˜¯å¦å­˜åœ¨\n",
    "                    if camera_name not in rgb_images_group:\n",
    "                        continue\n",
    "                    \n",
    "                    rgb_data = rgb_images_group[camera_name]\n",
    "                    \n",
    "                    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ\n",
    "                    if frame_idx >= len(rgb_data):\n",
    "                        continue\n",
    "                    \n",
    "                    # æå–å›¾åƒ\n",
    "                    if is_compress:\n",
    "                        # è§£ç å‹ç¼©å›¾åƒ\n",
    "                        compressed_img = rgb_data[frame_idx]\n",
    "                        img = self.decode_image(compressed_img)\n",
    "                        if img is not None:\n",
    "                            # è½¬æ¢BGRåˆ°RGB\n",
    "                            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                            images[frame_name] = img\n",
    "                    else:\n",
    "                        # ç›´æ¥è¯»å–æœªå‹ç¼©å›¾åƒ\n",
    "                        images[frame_name] = rgb_data[frame_idx]\n",
    "                        \n",
    "                except (ValueError, IndexError) as e:\n",
    "                    continue\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def save_images(self, images: Dict[str, np.ndarray], output_dir: Path):\n",
    "        \"\"\"\n",
    "        ä¿å­˜å›¾åƒåˆ°æŒ‡å®šç›®å½•\n",
    "        \n",
    "        Args:\n",
    "            images: å¸§åç§°åˆ°å›¾åƒæ•°ç»„çš„æ˜ å°„\n",
    "            output_dir: è¾“å‡ºç›®å½•\n",
    "        \"\"\"\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for frame_name, img in images.items():\n",
    "            output_path = output_dir / frame_name\n",
    "            # è½¬æ¢RGBå›BGRç”¨äºOpenCVä¿å­˜\n",
    "            img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(str(output_path), img_bgr)\n",
    "    \n",
    "    def process_jsonl(self, jsonl_path: str, limit: int = None):\n",
    "        \"\"\"\n",
    "        å¤„ç†JSONLæ–‡ä»¶å¹¶æå–æ‰€æœ‰å›¾åƒ\n",
    "        \n",
    "        Args:\n",
    "            jsonl_path: JSONLæ–‡ä»¶è·¯å¾„\n",
    "            limit: é™åˆ¶å¤„ç†çš„è¡Œæ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰\n",
    "        \"\"\"\n",
    "        # é‡ç½®é”™è¯¯åˆ—è¡¨\n",
    "        self.errors = []\n",
    "        \n",
    "        # æ”¶é›†æ‰€æœ‰éœ€è¦æå–çš„æ•°æ®\n",
    "        extraction_tasks = defaultdict(lambda: defaultdict(set))\n",
    "        \n",
    "        print(f\"ğŸ“– è¯»å–JSONLæ–‡ä»¶: {jsonl_path}\")\n",
    "        \n",
    "        # è¯»å–JSONLæ–‡ä»¶\n",
    "        with open(jsonl_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if limit:\n",
    "                lines = lines[:limit]\n",
    "                \n",
    "            for line_num, line in enumerate(lines, 1):\n",
    "                try:\n",
    "                    data = json.loads(line.strip())\n",
    "                    id_string = data['id']\n",
    "                    \n",
    "                    # æ”¶é›†æ‰€æœ‰éœ€è¦çš„å¸§\n",
    "                    all_frames = set()\n",
    "                    \n",
    "                    # visual_demoä¸­çš„å¸§\n",
    "                    if 'visual_demo' in data:\n",
    "                        all_frames.update(data['visual_demo'])\n",
    "                    \n",
    "                    # stage_to_estimateä¸­çš„å¸§\n",
    "                    if 'stage_to_estimate' in data:\n",
    "                        all_frames.update(data['stage_to_estimate'])\n",
    "                    \n",
    "                    # æŒ‰IDå’Œç›¸æœºåˆ†ç»„\n",
    "                    extraction_tasks[id_string]['frames'].update(all_frames)\n",
    "                    \n",
    "                except json.JSONDecodeError:\n",
    "                    self.errors.append(f\"ç¬¬ {line_num} è¡Œ: JSONè§£æé”™è¯¯\")\n",
    "                except Exception as e:\n",
    "                    self.errors.append(f\"ç¬¬ {line_num} è¡Œ: {str(e)}\")\n",
    "        \n",
    "        print(f\"ğŸ“Š å‘ç° {len(extraction_tasks)} ä¸ªå”¯ä¸€IDéœ€è¦å¤„ç†\\n\")\n",
    "        \n",
    "        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥\n",
    "        success_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        # ä½¿ç”¨tqdmå¤„ç†æ¯ä¸ªå”¯ä¸€çš„ID\n",
    "        with tqdm(total=len(extraction_tasks), desc=\"æå–å›¾åƒ\", unit=\"ID\") as pbar:\n",
    "            for id_string, task_info in extraction_tasks.items():\n",
    "                try:\n",
    "                    # è§£æID\n",
    "                    id_info = self.parse_id(id_string)\n",
    "                    \n",
    "                    # æ„å»ºHDF5è·¯å¾„\n",
    "                    h5_path = self.construct_h5_path(id_info)\n",
    "                    \n",
    "                    # æå–æ‰€æœ‰å¸§\n",
    "                    frames_to_extract = list(task_info['frames'])\n",
    "                    \n",
    "                    # ä»HDF5æå–å›¾åƒ\n",
    "                    images = self.extract_specific_frames(h5_path, frames_to_extract)\n",
    "                    \n",
    "                    if images:\n",
    "                        # æ„å»ºè¾“å‡ºè·¯å¾„\n",
    "                        output_dir = self.output_base_path / id_info['embodiment'] / \\\n",
    "                                    id_info['task_name'] / id_info['episode_id']\n",
    "                        \n",
    "                        # ä¿å­˜å›¾åƒ\n",
    "                        self.save_images(images, output_dir)\n",
    "                        success_count += 1\n",
    "                        \n",
    "                        # æ›´æ–°è¿›åº¦æ¡æè¿°\n",
    "                        pbar.set_postfix({\n",
    "                            'æˆåŠŸ': success_count, \n",
    "                            'å¤±è´¥': failed_count,\n",
    "                            'å½“å‰': f\"{id_info['task_name'][:20]}...\"\n",
    "                        })\n",
    "                    else:\n",
    "                        failed_count += 1\n",
    "                        self.errors.append(f\"ID {id_string}: æœªèƒ½æå–ä»»ä½•å›¾åƒ\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    failed_count += 1\n",
    "                    self.errors.append(f\"ID {id_string}: {str(e)}\")\n",
    "                    pbar.set_postfix({'æˆåŠŸ': success_count, 'å¤±è´¥': failed_count})\n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡\n",
    "        print(f\"\\nâœ… å¤„ç†å®Œæˆï¼\")\n",
    "        print(f\"   æˆåŠŸ: {success_count} ä¸ªID\")\n",
    "        print(f\"   å¤±è´¥: {failed_count} ä¸ªID\")\n",
    "        \n",
    "        # å¦‚æœæœ‰é”™è¯¯ï¼Œè¯¢é—®æ˜¯å¦æŸ¥çœ‹\n",
    "        if self.errors:\n",
    "            print(f\"\\nâš ï¸ å‘ç° {len(self.errors)} ä¸ªé”™è¯¯\")\n",
    "            response = input(\"æ˜¯å¦æŸ¥çœ‹é”™è¯¯è¯¦æƒ…ï¼Ÿ(y/n): \")\n",
    "            if response.lower() == 'y':\n",
    "                print(\"\\né”™è¯¯åˆ—è¡¨:\")\n",
    "                for i, error in enumerate(self.errors, 1):\n",
    "                    print(f\"  {i}. {error}\")\n",
    "    \n",
    "    def diagnose_path(self, id_string: str):\n",
    "        \"\"\"\n",
    "        è¯Šæ–­è·¯å¾„é—®é¢˜ï¼Œå¸®åŠ©è°ƒè¯•\n",
    "        \n",
    "        Args:\n",
    "            id_string: IDå­—ç¬¦ä¸²\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ” è¯Šæ–­ID: {id_string}\")\n",
    "        \n",
    "        try:\n",
    "            id_info = self.parse_id(id_string)\n",
    "            print(f\"  è§£æç»“æœ:\")\n",
    "            print(f\"    - Embodiment: {id_info['embodiment']}\")\n",
    "            print(f\"    - Task: {id_info['task_name']}\")\n",
    "            print(f\"    - Episode: {id_info['episode_id']}\")\n",
    "            \n",
    "            # æ£€æŸ¥embodimentç›®å½•\n",
    "            embodiment_path = self.base_data_path / id_info['embodiment']\n",
    "            print(f\"\\n  æ£€æŸ¥embodimentç›®å½•: {embodiment_path}\")\n",
    "            if embodiment_path.exists():\n",
    "                print(f\"    âœ“ ç›®å½•å­˜åœ¨\")\n",
    "                \n",
    "                # åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡\n",
    "                task_dirs = sorted([d.name for d in embodiment_path.iterdir() if d.is_dir()])\n",
    "                print(f\"    ğŸ“ æ‰¾åˆ° {len(task_dirs)} ä¸ªä»»åŠ¡ç›®å½•\")\n",
    "                \n",
    "                # æŸ¥æ‰¾ç›¸ä¼¼çš„ä»»åŠ¡å\n",
    "                from difflib import get_close_matches\n",
    "                similar = get_close_matches(id_info['task_name'], task_dirs, n=3, cutoff=0.6)\n",
    "                if similar:\n",
    "                    print(f\"    ğŸ’¡ ç›¸ä¼¼çš„ä»»åŠ¡å: {similar}\")\n",
    "                \n",
    "                # æ£€æŸ¥ç¡®åˆ‡çš„ä»»åŠ¡ç›®å½•\n",
    "                task_path = embodiment_path / id_info['task_name']\n",
    "                if task_path.exists():\n",
    "                    print(f\"\\n  âœ“ ä»»åŠ¡ç›®å½•å­˜åœ¨: {task_path}\")\n",
    "                    \n",
    "                    # æ£€æŸ¥episodeè·¯å¾„\n",
    "                    episode_path = task_path / 'success_episodes' / 'train' / id_info['episode_id']\n",
    "                    if episode_path.exists():\n",
    "                        print(f\"  âœ“ Episodeç›®å½•å­˜åœ¨: {episode_path}\")\n",
    "                        \n",
    "                        # æ£€æŸ¥dataç›®å½•\n",
    "                        data_path = episode_path / 'data'\n",
    "                        if data_path.exists():\n",
    "                            print(f\"  âœ“ Dataç›®å½•å­˜åœ¨: {data_path}\")\n",
    "                            \n",
    "                            # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶\n",
    "                            files = list(data_path.iterdir())\n",
    "                            print(f\"  ğŸ“ ç›®å½•å†…å®¹:\")\n",
    "                            for f in files:\n",
    "                                print(f\"      - {f.name} ({f.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "                            \n",
    "                            # æŸ¥æ‰¾HDF5æ–‡ä»¶\n",
    "                            h5_files = list(data_path.glob('*.hdf5'))\n",
    "                            if h5_files:\n",
    "                                print(f\"  âœ… æ‰¾åˆ°HDF5æ–‡ä»¶: {h5_files[0].name}\")\n",
    "                            else:\n",
    "                                print(f\"  âŒ æœªæ‰¾åˆ°HDF5æ–‡ä»¶\")\n",
    "                        else:\n",
    "                            print(f\"  âŒ Dataç›®å½•ä¸å­˜åœ¨\")\n",
    "                    else:\n",
    "                        print(f\"  âŒ Episodeç›®å½•ä¸å­˜åœ¨\")\n",
    "                        # åˆ—å‡ºå¯ç”¨çš„episodes\n",
    "                        train_path = task_path / 'success_episodes' / 'train'\n",
    "                        if train_path.exists():\n",
    "                            episodes = sorted([d.name for d in train_path.iterdir() if d.is_dir()])[:5]\n",
    "                            print(f\"  ğŸ’¡ å¯ç”¨çš„episodesç¤ºä¾‹: {episodes}\")\n",
    "                else:\n",
    "                    print(f\"  âŒ ä»»åŠ¡ç›®å½•ä¸å­˜åœ¨: {id_info['task_name']}\")\n",
    "                    print(f\"  ğŸ’¡ è¯·æ£€æŸ¥ä»»åŠ¡åæ˜¯å¦æ­£ç¡®\")\n",
    "            else:\n",
    "                print(f\"    âŒ Embodimentç›®å½•ä¸å­˜åœ¨\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ è¯Šæ–­å¤±è´¥: {e}\")\n",
    "    \n",
    "    def list_available_tasks(self, embodiment: str = \"h5_franka_3rgb\"):\n",
    "        \"\"\"\n",
    "        åˆ—å‡ºæŒ‡å®šembodimentä¸‹çš„æ‰€æœ‰å¯ç”¨ä»»åŠ¡\n",
    "        \n",
    "        Args:\n",
    "            embodiment: embodimentåç§°\n",
    "        \"\"\"\n",
    "        embodiment_path = self.base_data_path / embodiment\n",
    "        \n",
    "        if not embodiment_path.exists():\n",
    "            print(f\"âŒ Embodimentç›®å½•ä¸å­˜åœ¨: {embodiment_path}\")\n",
    "            return []\n",
    "        \n",
    "        tasks = []\n",
    "        for task_dir in sorted(embodiment_path.iterdir()):\n",
    "            if task_dir.is_dir():\n",
    "                # æ£€æŸ¥æ˜¯å¦æœ‰success_episodes/trainç›®å½•\n",
    "                train_path = task_dir / 'success_episodes' / 'train'\n",
    "                if train_path.exists():\n",
    "                    episodes = list(train_path.iterdir())\n",
    "                    if episodes:\n",
    "                        tasks.append({\n",
    "                            'name': task_dir.name,\n",
    "                            'episodes': len(episodes)\n",
    "                        })\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ {embodiment} ä¸‹çš„å¯ç”¨ä»»åŠ¡:\")\n",
    "        print(f\"{'ä»»åŠ¡åç§°':<50} {'Episodesæ•°é‡':>15}\")\n",
    "        print(\"-\" * 65)\n",
    "        for task in tasks:\n",
    "            print(f\"{task['name']:<50} {task['episodes']:>15}\")\n",
    "        \n",
    "        return tasks\n",
    "\n",
    "\n",
    "# ========== ä½¿ç”¨ç¤ºä¾‹ ==========\n",
    "\n",
    "def main():\n",
    "    # åˆ›å»ºæå–å™¨\n",
    "    extractor = ImageExtractor(\n",
    "        base_data_path=\"/home/runsheng/personal_3/qiancx/Sources/datasets/robomind/benchmark1_0_compressed\",\n",
    "        output_base_path=\"/home/runsheng/personal_3/qiancx/Sources/datasets/robomind/images\"\n",
    "    )\n",
    "    \n",
    "    # å¤„ç†JSONLæ–‡ä»¶\n",
    "    jsonl_file = \"/home/runsheng/personal_3/qiancx/ProgressLM/data/raw/converted/h5_agilex_3rgb_converted.jsonl\"\n",
    "    extractor.process_jsonl(jsonl_file)\n",
    "    \n",
    "    # å¦‚æœéœ€è¦è¯Šæ–­ç‰¹å®šIDçš„é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ï¼š\n",
    "    # extractor.diagnose_path(\"h5_agilex_3rgb/10_packplate_2/2024_09_28-16_27_42-172863566445571200.00\")\n",
    "    \n",
    "    # åˆ—å‡ºå¯ç”¨ä»»åŠ¡ï¼š\n",
    "    # extractor.list_available_tasks(\"h5_agilex_3rgb\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ff5bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import numpy as np\n",
    "\n",
    "# h5_path = \"/home/runsheng/personal_3/qiancx/Sources/datasets/robomind/benchmark1_0_compressed/h5_agilex_3rgb/10_packplate_2/success_episodes/train/2024_09_28-16_27_42-172863566445571200.00/data/trajectory.hdf5\"\n",
    "\n",
    "# def explore_h5_structure(path, max_depth=4):\n",
    "#     \"\"\"é€’å½’æ¢ç´¢HDF5æ–‡ä»¶ç»“æ„\"\"\"\n",
    "    \n",
    "#     def print_structure(name, obj, depth=0):\n",
    "#         if depth > max_depth:\n",
    "#             return\n",
    "        \n",
    "#         indent = \"  \" * depth\n",
    "        \n",
    "#         if isinstance(obj, h5py.Group):\n",
    "#             print(f\"{indent}ğŸ“ {name}/ (Group)\")\n",
    "#             # æ‰“å°å±æ€§\n",
    "#             if obj.attrs:\n",
    "#                 for key, val in obj.attrs.items():\n",
    "#                     print(f\"{indent}   @{key} = {val}\")\n",
    "        \n",
    "#         elif isinstance(obj, h5py.Dataset):\n",
    "#             print(f\"{indent}ğŸ“„ {name} (Dataset)\")\n",
    "#             print(f\"{indent}   Shape: {obj.shape}\")\n",
    "#             print(f\"{indent}   Dtype: {obj.dtype}\")\n",
    "#             if obj.attrs:\n",
    "#                 for key, val in obj.attrs.items():\n",
    "#                     print(f\"{indent}   @{key} = {val}\")\n",
    "    \n",
    "#     print(f\"\\nğŸ” æ¢ç´¢HDF5æ–‡ä»¶ç»“æ„: {path}\\n\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     with h5py.File(path, 'r') as f:\n",
    "#         # æ‰“å°æ–‡ä»¶çº§åˆ«å±æ€§\n",
    "#         print(\"ğŸ“‹ æ–‡ä»¶å±æ€§:\")\n",
    "#         for key, val in f.attrs.items():\n",
    "#             print(f\"  @{key} = {val}\")\n",
    "#         print()\n",
    "        \n",
    "#         # é€’å½’æ‰“å°ç»“æ„\n",
    "#         f.visititems(print_structure)\n",
    "        \n",
    "#         # ç‰¹åˆ«å…³æ³¨rgb_images\n",
    "#         print(\"\\n\" + \"=\" * 80)\n",
    "#         print(\"ğŸ¯ é‡ç‚¹æ£€æŸ¥: RGBå›¾åƒè·¯å¾„\")\n",
    "#         print(\"=\" * 80)\n",
    "        \n",
    "#         # å°è¯•å¸¸è§è·¯å¾„\n",
    "#         paths_to_check = [\n",
    "#             'observations/rgb_images',\n",
    "#             'observations/images',\n",
    "#             'rgb_images',\n",
    "#             'images',\n",
    "#             'obs/rgb_images',\n",
    "#             'obs/images',\n",
    "#         ]\n",
    "        \n",
    "#         for path_str in paths_to_check:\n",
    "#             if path_str in f:\n",
    "#                 print(f\"\\nâœ… æ‰¾åˆ°: {path_str}\")\n",
    "#                 obj = f[path_str]\n",
    "#                 if isinstance(obj, h5py.Group):\n",
    "#                     print(f\"   ç›¸æœºåˆ—è¡¨: {list(obj.keys())}\")\n",
    "#                     for camera in obj.keys():\n",
    "#                         print(f\"     - {camera}: shape={obj[camera].shape}, dtype={obj[camera].dtype}\")\n",
    "#                 else:\n",
    "#                     print(f\"   Shape: {obj.shape}, Dtype: {obj.dtype}\")\n",
    "#             else:\n",
    "#                 print(f\"âŒ æœªæ‰¾åˆ°: {path_str}\")\n",
    "\n",
    "# # è¿è¡Œæ¢ç´¢\n",
    "# explore_h5_structure(h5_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
